# ML Dimensionality Reduction

> The method of reducing the dimension of your feature set is known as dimensionality reduction. Your feature set may be a hundred-column dataset (i.e features).

- Why would we drop features (columns)?
  - The Curse of Dimensionality :
  - The more features there are, the more difficult it is to visualize the training set and then work on it.
  - The model becomes more complex as the number of features increases. Overfitting is more likely as the number of features increases, resulting in poor output on real data. 

- Feature selection is the process of identifying and selecting relevant features for your sample. Feature selection can be done either manually or programmatically. Feature selection is the simplest of dimensionality reduction methods.

- Linear Dimensionality Reduction Methods
  1. Principal Component Analysis (PCA)
  2. Linear Discriminant Analysis (LDA)
  3. Factor Analysis
  4. etc.

- Non-linear Dimensionality Reduction Methods
  1. Kernel PCA 
  2. Multi-dimensional scaling (MDS)
  3. Isometric Feature Mapping (Isomap)
  4.  etc.

  ---

### If you like my work, you can contribute to https://www.patreon.com/xscotophilic

### Thank You!